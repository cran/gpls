%
% NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is
% likely to be overwritten.
%
% \VignetteIndexEntry{gpls Tutorial}
% \VignetteDepends{}
% \VignetteKeywords{}
% \VignettePackage{gpls}
\documentclass[11pt]{article}

\usepackage{amsmath,epsfig,psfig,fullpage}
\usepackage{graphicx}
\usepackage[authoryear,round]{natbib}
\usepackage{hyperref}

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}

%\parindent 0in

%\bibliographystyle{plainnat}

\usepackage{/home/jgentry/R/share/texmf/Sweave}
\begin{document}
\title{Classification using Generalized Partial Least Squares}
\author{Beiying Ding\\ Robert Gentleman}
\maketitle

\section*{Introduction}
The \Rpackage{gpls} package includes functions for classification using
generalized partial least squares approaches. Both two-group and
multi-group (more than 2 groups) classifications can be done. The
basic functionalities are based on and extended from the Iteratively
ReWeighted Least Squares (IRWPLS) by \citet{marx:1996}. Additionally,
Firth's bias reduction procedure \citep{firth:1992:a, firth:1992:b,
  firth:1993} is incorporated to remedy the nonconvergence problem
frequently encountered in logistic regression. For more detailed
description of classification using generalized partial least squares,
refer to \citet{ding:2003:c}.\\

\section*{The \Rfunction{glpls1a} function}
The \Rfunction{glpls1a} function carries out two-group classification via
IRWPLS(F). Whether or not to use Firth's bias reduction is an option
({\tt br=T}). The X matrix shouldn't include an intercept term.\\

\begin{Schunk}
\begin{Sinput}
> library(gpls)
> set.seed(123)
> x <- matrix(rnorm(20), ncol = 2)
> y <- sample(0:1, 10, TRUE)
> glpls1a(x, y, br = FALSE)
\end{Sinput}
\begin{Soutput}
$coefficients
[1] -2.3121694 -0.5069340 -0.4528994

$convergence
[1] TRUE

$niter
[1] 7

$bias.reduction
[1] FALSE
\end{Soutput}
\begin{Sinput}
> glpls1a(x, y, K.prov = 1, br = FALSE)
\end{Sinput}
\begin{Soutput}
$coefficients
[1] -2.2916025 -0.3650184 -0.5377371

$convergence
[1] TRUE

$niter
[1] 6

$bias.reduction
[1] FALSE
\end{Soutput}
\begin{Sinput}
> glpls1a(x, y, br = TRUE)
\end{Sinput}
\begin{Soutput}
$coefficients
[1] -1.4319250 -0.1549106 -0.2760085

$convergence
[1] TRUE

$niter
[1] 11

$bias.reduction
[1] TRUE
\end{Soutput}
\end{Schunk}
{\tt K.prov} specifies the number PLS components to use. Note that
when {\tt K.prov} is no specified, the number of PLS components are
set to be the smaller of the row and column rank of the design
matrix.\\

\section*{The \Rfunction{glpls1a.cv.error} and
          \Rfunction{glpls1a.train.test.error} functions}
The \Rfunction{glpls1a.cv.error} calculates leave-one-out classification
error rate for two-group classification and
\texttt{glpls1a.train.test.error}
calculates test set error where the model is fit using the training
set.\\

\begin{Schunk}
\begin{Sinput}
> x <- matrix(rnorm(20), ncol = 2)
> y <- sample(0:1, 10, TRUE)
> x1 <- matrix(rnorm(10), ncol = 2)
> y1 <- sample(0:1, 5, TRUE)
> glpls1a.cv.error(x, y, br = FALSE)
\end{Sinput}
\begin{Soutput}
$error
[1] 0.6

$error.obs
[1]  1  2  4  7  9 10
\end{Soutput}
\begin{Sinput}
> glpls1a.train.test.error(x, y, x1, y1, br = FALSE)
\end{Sinput}
\begin{Soutput}
$error
[1] 0.2

$error.obs
[1] 1

$predict.test
          [,1]
[1,] 0.4871740
[2,] 0.3041578
[3,] 0.4454992
[4,] 0.3082741
[5,] 0.4344300
\end{Soutput}
\begin{Sinput}
> glpls1a.cv.error(x, y, K.prov = 1, br = TRUE)
\end{Sinput}
\begin{Soutput}
$error
[1] 0.6

$error.obs
[1]  1  2  4  7  9 10
\end{Soutput}
\begin{Sinput}
> glpls1a.train.test.error(x, y, x1, y1, K.prov = 1, br = TRUE)
\end{Sinput}
\begin{Soutput}
$error
[1] 0.2

$error.obs
[1] 1

$predict.test
          [,1]
[1,] 0.4828225
[2,] 0.3455864
[3,] 0.4520410
[4,] 0.3497578
[5,] 0.4437634
\end{Soutput}
\end{Schunk}
\section*{The \texttt{glpls1a.mlogit} and \texttt{glpls1a.logit.all} functions}
The \Rfunction{glpls1a.mlogit} carries out multi-group classification using
MIRWPLS(F) where the baseline logit model is used as counterpart to
\Rfunction{glpls1a} for two group case. \Rfunction{glpls1a.logit.all}
carries out
multi-group classification by separately fitting $C$ two-group
classification using \Rfunction{glpls1a} separately for $C$ group vs the
same baseline class (i.e. altogether $C+1$ classes). This separate
fitting of logit is known to be less efficient but has been used in
practice due to its more straightforward implementation.\\

Note that when using \Rfunction{glpls1a.mlogit}, the X matrix needs to have
a column of one, i.e. intercept term.

\begin{Schunk}
\begin{Sinput}
> x <- matrix(rnorm(20), ncol = 2)
> y <- sample(1:3, 10, TRUE)
> glpls1a.mlogit(cbind(rep(1, 10), x), y, K.prov = 1, br = FALSE)
\end{Sinput}
\begin{Soutput}
$coefficients
           [,1]       [,2]
[1,] -0.7689747 -0.0370627
[2,]  2.1926005 -4.2015007
[3,]  0.5442713 -0.9331431

$convergence
[1] FALSE

$niter
[1] 100

$bias.reduction
[1] FALSE
\end{Soutput}
\begin{Sinput}
> glpls1a.logit.all(x, y, K.prov = 1, br = FALSE)
\end{Sinput}
\begin{Soutput}
$coefficients
          [,1]      [,2]
[1,] -2.339152  1.029019
[2,]  2.906929 -9.000959
[3,]  0.579930 -1.387408
\end{Soutput}
\begin{Sinput}
> glpls1a.mlogit(cbind(rep(1, 10), x), y, br = TRUE)
\end{Sinput}
\begin{Soutput}
$coefficients
           [,1]        [,2]
[1,] -1.0327659  0.41635681
[2,]  1.2298647 -2.58869374
[3,]  0.4357512 -0.08656436

$convergence
[1] TRUE

$niter
[1] 36

$bias.reduction
[1] TRUE
\end{Soutput}
\begin{Sinput}
> glpls1a.logit.all(x, y, br = TRUE)
\end{Sinput}
\begin{Soutput}
$coefficients
           [,1]       [,2]
[1,] -1.1433739  0.5402162
[2,]  1.3337074 -2.9934497
[3,]  0.5325446 -0.2234683
\end{Soutput}
\end{Schunk}
\section*{The \Rfunction{glpls1a.mlogit.cv.error} function}
The \Rfunction{glpls1a.mlogit.cv.error} calculates leave-one-out error for
multi-group classification using (M)IRWPLS(F). When the \Robject{mlogit}
option is set to be true, then \Rfunction{glpls1a.mlogit} is used, else
\Rfunction{glpls1a.logit.all} is used for fitting.\\

\begin{Schunk}
\begin{Sinput}
> x <- matrix(rnorm(20), ncol = 2)
> y <- sample(1:3, 10, TRUE)
> glpls1a.mlogit.cv.error(x, y, br = FALSE)
\end{Sinput}
\begin{Soutput}
$error
[1] 0.6

$error.obs
[1]  3  4  5  7  9 10
\end{Soutput}
\begin{Sinput}
> glpls1a.mlogit.cv.error(x, y, mlogit = FALSE, br = FALSE)
\end{Sinput}
\begin{Soutput}
$error
[1] 0.5

$error.obs
[1]  3  4  5  7 10
\end{Soutput}
\begin{Sinput}
> glpls1a.mlogit.cv.error(x, y, br = TRUE)
\end{Sinput}
\begin{Soutput}
$error
[1] 0.6

$error.obs
[1]  3  4  5  7  9 10
\end{Soutput}
\begin{Sinput}
> glpls1a.mlogit.cv.error(x, y, mlogit = FALSE, br = TRUE)
\end{Sinput}
\begin{Soutput}
$error
[1] 0.5

$error.obs
[1]  3  4  5  7 10
\end{Soutput}
\end{Schunk}

%\bibliography{gpls}
\end{document}
